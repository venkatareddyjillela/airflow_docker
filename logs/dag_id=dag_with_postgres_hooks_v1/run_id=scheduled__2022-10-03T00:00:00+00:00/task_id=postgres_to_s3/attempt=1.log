[2022-10-04T12:16:36.479+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:16:36.513+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:16:36.514+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:16:36.514+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:16:36.515+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:16:36.566+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-03 00:00:00+00:00
[2022-10-04T12:16:36.576+0000] {standard_task_runner.py:54} INFO - Started process 14419 to run task
[2022-10-04T12:16:36.585+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-03T00:00:00+00:00', '--job-id', '107', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmphd0s4791']
[2022-10-04T12:16:36.588+0000] {standard_task_runner.py:83} INFO - Job 107: Subtask postgres_to_s3
[2022-10-04T12:16:36.591+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:16:37.108+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:16:37.336+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-03T00:00:00+00:00
[2022-10-04T12:16:37.360+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:16:37.408+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag_with_postgres_hooks.py", line 19, in postgres_to_s3
    cursor.execute("select * from order where data <= 20220501")
psycopg2.errors.SyntaxError: syntax error at or near "order"
LINE 1: select * from order where data <= 20220501
                      ^

[2022-10-04T12:16:37.447+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221003T000000, start_date=20221004T121636, end_date=20221004T121637
[2022-10-04T12:16:37.493+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 107 for task postgres_to_s3 (syntax error at or near "order"
LINE 1: select * from order where data <= 20220501
                      ^
; 14419)
[2022-10-04T12:16:37.524+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-04T12:16:37.594+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:20:06.328+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:20:06.345+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:20:06.345+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:20:06.345+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:20:06.346+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:20:06.376+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-03 00:00:00+00:00
[2022-10-04T12:20:06.383+0000] {standard_task_runner.py:54} INFO - Started process 14858 to run task
[2022-10-04T12:20:06.389+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-03T00:00:00+00:00', '--job-id', '110', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpa9dyy6qp']
[2022-10-04T12:20:06.391+0000] {standard_task_runner.py:83} INFO - Job 110: Subtask postgres_to_s3
[2022-10-04T12:20:06.393+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:20:06.695+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:20:06.796+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-03T00:00:00+00:00
[2022-10-04T12:20:06.807+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:20:06.824+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag_with_postgres_hooks.py", line 19, in postgres_to_s3
    cursor.execute("select * from orders where data <= '20220501'")
psycopg2.errors.UndefinedColumn: column "data" does not exist
LINE 1: select * from orders where data <= '20220501'
                                   ^
HINT:  Perhaps you meant to reference the column "orders.date".

[2022-10-04T12:20:06.835+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221003T000000, start_date=20221004T122006, end_date=20221004T122006
[2022-10-04T12:20:06.854+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 110 for task postgres_to_s3 (column "data" does not exist
LINE 1: select * from orders where data <= '20220501'
                                   ^
HINT:  Perhaps you meant to reference the column "orders.date".
; 14858)
[2022-10-04T12:20:06.882+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-04T12:20:06.906+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:21:06.977+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:21:07.004+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:21:07.004+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:21:07.005+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:21:07.005+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:21:07.045+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-03 00:00:00+00:00
[2022-10-04T12:21:07.054+0000] {standard_task_runner.py:54} INFO - Started process 14989 to run task
[2022-10-04T12:21:07.061+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-03T00:00:00+00:00', '--job-id', '112', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmp6tjtsu0m']
[2022-10-04T12:21:07.064+0000] {standard_task_runner.py:83} INFO - Job 112: Subtask postgres_to_s3
[2022-10-04T12:21:07.067+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:21:07.552+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:21:07.733+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-03T00:00:00+00:00
[2022-10-04T12:21:07.755+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:21:07.794+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag_with_postgres_hooks.py", line 20, in postgres_to_s3
    with open('/dags/get_orders.txt', 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/dags/get_orders.txt'
[2022-10-04T12:21:07.815+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221003T000000, start_date=20221004T122106, end_date=20221004T122107
[2022-10-04T12:21:07.848+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 112 for task postgres_to_s3 ([Errno 2] No such file or directory: '/dags/get_orders.txt'; 14989)
[2022-10-04T12:21:07.876+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-04T12:21:07.918+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:23:00.511+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:23:00.534+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:23:00.534+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:23:00.535+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:23:00.535+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:23:00.571+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-03 00:00:00+00:00
[2022-10-04T12:23:00.579+0000] {standard_task_runner.py:54} INFO - Started process 15229 to run task
[2022-10-04T12:23:00.585+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-03T00:00:00+00:00', '--job-id', '114', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmp9ohhtkpi']
[2022-10-04T12:23:00.588+0000] {standard_task_runner.py:83} INFO - Job 114: Subtask postgres_to_s3
[2022-10-04T12:23:00.591+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:23:01.072+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:23:01.389+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-03T00:00:00+00:00
[2022-10-04T12:23:01.417+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:23:01.472+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag_with_postgres_hooks.py", line 20, in postgres_to_s3
    with open('/dags/get_orders.txt', 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/dags/get_orders.txt'
[2022-10-04T12:23:01.502+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221003T000000, start_date=20221004T122300, end_date=20221004T122301
[2022-10-04T12:23:01.540+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 114 for task postgres_to_s3 ([Errno 2] No such file or directory: '/dags/get_orders.txt'; 15229)
[2022-10-04T12:23:01.571+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-04T12:23:01.632+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:25:11.582+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:25:11.612+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:25:11.612+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:25:11.613+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:25:11.613+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:25:11.660+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-03 00:00:00+00:00
[2022-10-04T12:25:11.669+0000] {standard_task_runner.py:54} INFO - Started process 15503 to run task
[2022-10-04T12:25:11.680+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-03T00:00:00+00:00', '--job-id', '116', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpoz89q5tp']
[2022-10-04T12:25:11.683+0000] {standard_task_runner.py:83} INFO - Job 116: Subtask postgres_to_s3
[2022-10-04T12:25:11.685+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:25:12.230+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:25:12.403+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-03T00:00:00+00:00
[2022-10-04T12:25:12.429+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:25:12.471+0000] {dag_with_postgres_hooks.py:27} INFO - saved orders data to file
[2022-10-04T12:25:12.472+0000] {python.py:177} INFO - Done. Returned value was: None
[2022-10-04T12:25:12.498+0000] {taskinstance.py:1406} INFO - Marking task as SUCCESS. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221003T000000, start_date=20221004T122511, end_date=20221004T122512
[2022-10-04T12:25:12.572+0000] {local_task_job.py:164} INFO - Task exited with return code 0
[2022-10-04T12:25:12.626+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:36:00.818+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:36:00.846+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [queued]>
[2022-10-04T12:36:00.846+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:36:00.847+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:36:00.847+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:36:00.885+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-03 00:00:00+00:00
[2022-10-04T12:36:00.896+0000] {standard_task_runner.py:54} INFO - Started process 16960 to run task
[2022-10-04T12:36:00.903+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-03T00:00:00+00:00', '--job-id', '119', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmp3p8m5acr']
[2022-10-04T12:36:00.906+0000] {standard_task_runner.py:83} INFO - Job 119: Subtask postgres_to_s3
[2022-10-04T12:36:00.909+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:36:01.414+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-03T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:36:01.628+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-03T00:00:00+00:00
[2022-10-04T12:36:01.643+0000] {logging_mixin.py:117} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:204 AirflowContextDeprecationWarning: Accessing 'next_ds_nodash' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds_nodash }}' instead.
[2022-10-04T12:36:01.663+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:36:01.696+0000] {dag_with_postgres_hooks.py:28} INFO - saved orders data to file : dags/get_orders_20221003.txt
[2022-10-04T12:36:01.697+0000] {python.py:177} INFO - Done. Returned value was: None
[2022-10-04T12:36:01.725+0000] {taskinstance.py:1406} INFO - Marking task as SUCCESS. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221003T000000, start_date=20221004T123600, end_date=20221004T123601
[2022-10-04T12:36:01.798+0000] {local_task_job.py:164} INFO - Task exited with return code 0
[2022-10-04T12:36:01.844+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
