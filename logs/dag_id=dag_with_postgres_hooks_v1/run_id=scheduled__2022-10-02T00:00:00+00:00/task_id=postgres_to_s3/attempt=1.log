[2022-10-04T12:16:36.222+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:16:36.253+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:16:36.254+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:16:36.254+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:16:36.255+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:16:36.359+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-02 00:00:00+00:00
[2022-10-04T12:16:36.368+0000] {standard_task_runner.py:54} INFO - Started process 14418 to run task
[2022-10-04T12:16:36.378+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-02T00:00:00+00:00', '--job-id', '106', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpyki0c9_7']
[2022-10-04T12:16:36.382+0000] {standard_task_runner.py:83} INFO - Job 106: Subtask postgres_to_s3
[2022-10-04T12:16:36.384+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:16:36.946+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:16:37.150+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-02T00:00:00+00:00
[2022-10-04T12:16:37.175+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:16:37.210+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag_with_postgres_hooks.py", line 19, in postgres_to_s3
    cursor.execute("select * from order where data <= 20220501")
psycopg2.errors.SyntaxError: syntax error at or near "order"
LINE 1: select * from order where data <= 20220501
                      ^

[2022-10-04T12:16:37.236+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221002T000000, start_date=20221004T121636, end_date=20221004T121637
[2022-10-04T12:16:37.277+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 106 for task postgres_to_s3 (syntax error at or near "order"
LINE 1: select * from order where data <= 20220501
                      ^
; 14418)
[2022-10-04T12:16:37.312+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-04T12:16:37.373+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:20:06.241+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:20:06.259+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:20:06.259+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:20:06.260+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:20:06.260+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:20:06.288+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-02 00:00:00+00:00
[2022-10-04T12:20:06.294+0000] {standard_task_runner.py:54} INFO - Started process 14857 to run task
[2022-10-04T12:20:06.300+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-02T00:00:00+00:00', '--job-id', '109', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpjuowvt1o']
[2022-10-04T12:20:06.302+0000] {standard_task_runner.py:83} INFO - Job 109: Subtask postgres_to_s3
[2022-10-04T12:20:06.303+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:20:06.623+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:20:06.722+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-02T00:00:00+00:00
[2022-10-04T12:20:06.733+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:20:06.754+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag_with_postgres_hooks.py", line 19, in postgres_to_s3
    cursor.execute("select * from orders where data <= '20220501'")
psycopg2.errors.UndefinedColumn: column "data" does not exist
LINE 1: select * from orders where data <= '20220501'
                                   ^
HINT:  Perhaps you meant to reference the column "orders.date".

[2022-10-04T12:20:06.764+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221002T000000, start_date=20221004T122006, end_date=20221004T122006
[2022-10-04T12:20:06.784+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 109 for task postgres_to_s3 (column "data" does not exist
LINE 1: select * from orders where data <= '20220501'
                                   ^
HINT:  Perhaps you meant to reference the column "orders.date".
; 14857)
[2022-10-04T12:20:06.831+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-04T12:20:06.857+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:21:06.844+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:21:06.870+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:21:06.871+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:21:06.871+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:21:06.872+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:21:06.914+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-02 00:00:00+00:00
[2022-10-04T12:21:06.923+0000] {standard_task_runner.py:54} INFO - Started process 14988 to run task
[2022-10-04T12:21:06.931+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-02T00:00:00+00:00', '--job-id', '111', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpe0lwewqb']
[2022-10-04T12:21:06.935+0000] {standard_task_runner.py:83} INFO - Job 111: Subtask postgres_to_s3
[2022-10-04T12:21:06.937+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:21:07.408+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:21:07.573+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-02T00:00:00+00:00
[2022-10-04T12:21:07.593+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:21:07.637+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag_with_postgres_hooks.py", line 20, in postgres_to_s3
    with open('/dags/get_orders.txt', 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/dags/get_orders.txt'
[2022-10-04T12:21:07.656+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221002T000000, start_date=20221004T122106, end_date=20221004T122107
[2022-10-04T12:21:07.689+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 111 for task postgres_to_s3 ([Errno 2] No such file or directory: '/dags/get_orders.txt'; 14988)
[2022-10-04T12:21:07.746+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-04T12:21:07.806+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:23:00.396+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:23:00.422+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:23:00.422+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:23:00.423+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:23:00.423+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:23:00.463+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-02 00:00:00+00:00
[2022-10-04T12:23:00.471+0000] {standard_task_runner.py:54} INFO - Started process 15228 to run task
[2022-10-04T12:23:00.478+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-02T00:00:00+00:00', '--job-id', '113', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpd31hc00i']
[2022-10-04T12:23:00.481+0000] {standard_task_runner.py:83} INFO - Job 113: Subtask postgres_to_s3
[2022-10-04T12:23:00.483+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:23:00.942+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:23:01.114+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-02T00:00:00+00:00
[2022-10-04T12:23:01.135+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:23:01.172+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dag_with_postgres_hooks.py", line 20, in postgres_to_s3
    with open('/dags/get_orders.txt', 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/dags/get_orders.txt'
[2022-10-04T12:23:01.200+0000] {taskinstance.py:1406} INFO - Marking task as UP_FOR_RETRY. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221002T000000, start_date=20221004T122300, end_date=20221004T122301
[2022-10-04T12:23:01.240+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 113 for task postgres_to_s3 ([Errno 2] No such file or directory: '/dags/get_orders.txt'; 15228)
[2022-10-04T12:23:01.293+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-10-04T12:23:01.416+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:25:11.459+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:25:11.488+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:25:11.488+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:25:11.489+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:25:11.489+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:25:11.527+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-02 00:00:00+00:00
[2022-10-04T12:25:11.535+0000] {standard_task_runner.py:54} INFO - Started process 15502 to run task
[2022-10-04T12:25:11.543+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-02T00:00:00+00:00', '--job-id', '115', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpflmsn2_y']
[2022-10-04T12:25:11.546+0000] {standard_task_runner.py:83} INFO - Job 115: Subtask postgres_to_s3
[2022-10-04T12:25:11.548+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:25:12.185+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:25:12.361+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-02T00:00:00+00:00
[2022-10-04T12:25:12.384+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:25:12.432+0000] {dag_with_postgres_hooks.py:27} INFO - saved orders data to file
[2022-10-04T12:25:12.437+0000] {python.py:177} INFO - Done. Returned value was: None
[2022-10-04T12:25:12.465+0000] {taskinstance.py:1406} INFO - Marking task as SUCCESS. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221002T000000, start_date=20221004T122511, end_date=20221004T122512
[2022-10-04T12:25:12.518+0000] {local_task_job.py:164} INFO - Task exited with return code 0
[2022-10-04T12:25:12.567+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-10-04T12:36:00.685+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:36:00.716+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [queued]>
[2022-10-04T12:36:00.716+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:36:00.717+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 3
[2022-10-04T12:36:00.717+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-10-04T12:36:00.759+0000] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): postgres_to_s3> on 2022-10-02 00:00:00+00:00
[2022-10-04T12:36:00.767+0000] {standard_task_runner.py:54} INFO - Started process 16959 to run task
[2022-10-04T12:36:00.775+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dag_with_postgres_hooks_v1', 'postgres_to_s3', 'scheduled__2022-10-02T00:00:00+00:00', '--job-id', '118', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_postgres_hooks.py', '--cfg-path', '/tmp/tmpd3ochp7e']
[2022-10-04T12:36:00.777+0000] {standard_task_runner.py:83} INFO - Job 118: Subtask postgres_to_s3
[2022-10-04T12:36:00.780+0000] {dagbag.py:525} INFO - Filling up the DagBag from /opt/***/dags/dag_with_postgres_hooks.py
[2022-10-04T12:36:01.275+0000] {task_command.py:384} INFO - Running <TaskInstance: dag_with_postgres_hooks_v1.postgres_to_s3 scheduled__2022-10-02T00:00:00+00:00 [running]> on host fe59f3828ca7
[2022-10-04T12:36:01.471+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dag_with_postgres_hooks_v1
AIRFLOW_CTX_TASK_ID=postgres_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-10-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-10-02T00:00:00+00:00
[2022-10-04T12:36:01.489+0000] {logging_mixin.py:117} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:204 AirflowContextDeprecationWarning: Accessing 'next_ds_nodash' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds_nodash }}' instead.
[2022-10-04T12:36:01.512+0000] {base.py:71} INFO - Using connection ID 'postgres_localhost' for task execution.
[2022-10-04T12:36:01.544+0000] {dag_with_postgres_hooks.py:28} INFO - saved orders data to file : dags/get_orders_20221002.txt
[2022-10-04T12:36:01.545+0000] {python.py:177} INFO - Done. Returned value was: None
[2022-10-04T12:36:01.572+0000] {taskinstance.py:1406} INFO - Marking task as SUCCESS. dag_id=dag_with_postgres_hooks_v1, task_id=postgres_to_s3, execution_date=20221002T000000, start_date=20221004T123600, end_date=20221004T123601
[2022-10-04T12:36:01.630+0000] {local_task_job.py:164} INFO - Task exited with return code 0
[2022-10-04T12:36:01.678+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
